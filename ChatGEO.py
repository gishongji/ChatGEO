# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸŒ åœ°å½¢åœ°è²Œæ™ºèƒ½é—®ç­”åŠ©æ‰‹")

# æºå¤§æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = './IEITYuan/Yuan2-2B-Mars-hf'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16 # A10

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("Creating tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)
    
    # è®¾å®šæˆ–æ·»åŠ å¡«å……token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # é€‰æ‹©å°†`<eod>`ä½œä¸ºå¡«å……token
        # æˆ–è€…æ·»åŠ ä¸€ä¸ªæ–°çš„tokenä½œä¸ºå¡«å……token
        # tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    
    print("Creating model...")
    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()

    print("Done.")
    return tokenizer, model

# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()

# åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# æ¯æ¬¡å¯¹è¯æ—¶ï¼Œéƒ½éœ€è¦éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # æ·»åŠ åœ°å½¢åœ°è²Œé¢†åŸŸçš„ä¸Šä¸‹æ–‡
    domain_context = "æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„åœ°ç†å­¦å®¶ï¼Œä¸“é—¨ç ”ç©¶åœ°å½¢åœ°è²Œã€‚"

    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # æ‹¼æ¥å¯¹è¯å†å²
    prompt = domain_context + "<n>".join(msg["content"] for msg in st.session_state.messages) + "<sep>"
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1024)

    # è·å–`input_ids`å’Œ`attention_mask`
    input_ids = inputs["input_ids"].cuda()
    attention_mask = inputs["attention_mask"].cuda()

    outputs = model.generate(input_ids, attention_mask=attention_mask, do_sample=False, max_new_tokens=1024) # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '')

    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)