# ChatGEO
åŸºäºå¤§æ¨¡å‹çš„åœ°ç†çŸ¥è¯†é—®ç­”åŠ©æ‰‹

---
[toc]

---

## å‰è¨€

æ­¤æ¬¡å¼€å‘å°è¯•æ˜¯ä¸ºäº†å®Œæˆdatawhaleå¤ä»¤è¥ä¸­çš„â€œå¤§æ¨¡å‹åº”ç”¨å¼€å‘â€å®è·µï¼Œä»£ç å‚è€ƒäº†å®˜æ–¹çš„demoï¼Œåªæ˜¯è‡ªå·±ç¨ä½œä¿®æ”¹ï¼Œä¸å…·å¤‡çœŸæ­£æ„ä¹‰ä¸Šçš„â€œåœ°ç†çŸ¥è¯†åº“â€ï¼Œä»…ä»…é€šè¿‡é¢„å…ˆè°ƒæ•´æç¤ºè¯å¯¹å¤§æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œä¸€å®šæ§åˆ¶ï¼Œæœ¬è´¨è¿˜æ˜¯Yuan 2-2B Mars-HFå¯¹è¯å¤§æ¨¡å‹ã€‚

å¸Œæœ›ä»¥åå¯ä»¥é€æ­¥åŠ ä¸ŠRAG/LoRAç­‰æ–¹æ³•ï¼Œç»§ç»­å®Œå–„åº”ç”¨ï¼Œå¹¶æœé›†åœ°å½¢åœ°è²ŒçŸ¥è¯†é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå®Œæˆä¸€ä¸ªçœŸæ­£çš„åœ°ç†çŸ¥è¯†é—®ç­”å¤§è¯­è¨€æ¨¡å‹ã€‚

æ„Ÿè°¢datawhaleä¸é­”æ­ç¤¾åŒºçš„å„ç§æ”¯æŒï¼

---

## é¡¹ç›®èƒŒæ™¯

éšç€åœ°ç†ç§‘å­¦çš„è¿›æ­¥å’ŒæŠ€æœ¯çš„å‘å±•ï¼Œäººä»¬å¯¹åœ°å½¢åœ°è²Œçš„ç ”ç©¶è¶Šæ¥è¶Šæ·±å…¥ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ç ”ç©¶æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡çš„å®åœ°è€ƒå¯Ÿå’Œæ–‡çŒ®æŸ¥é˜…ï¼Œè¿™ä¸ä»…è€—æ—¶è€—åŠ›ï¼Œè€Œä¸”æ•ˆç‡è¾ƒä½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€æ¬¾åŸºäºå¤§æ¨¡å‹çš„åœ°å½¢åœ°è²Œæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œæ—¨åœ¨åˆ©ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯æå‡åœ°å½¢åœ°è²Œç ”ç©¶çš„æ•ˆç‡å’Œè´¨é‡ã€‚

---

## äº§å“åŠŸèƒ½

æœ¬äº§å“èƒ½å¤Ÿå®ç°ä»¥ä¸‹ä¸»è¦åŠŸèƒ½ï¼š

1. **æ™ºèƒ½é—®ç­”**ï¼šç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€å‘åŠ©æ‰‹æé—®æœ‰å…³åœ°å½¢åœ°è²Œçš„é—®é¢˜ï¼Œè·å¾—å¿«é€Ÿä¸”å‡†ç¡®çš„å›ç­”ã€‚
2. **é¢†åŸŸçŸ¥è¯†é›†æˆ**ï¼šå†…ç½®ä¸“ä¸šé¢†åŸŸçŸ¥è¯†åº“ï¼Œèƒ½å¤Ÿé’ˆå¯¹åœ°å½¢åœ°è²Œçš„ç›¸å…³é—®é¢˜ç»™å‡ºä¸“ä¸šè§£ç­”ã€‚
3. **äº¤äº’å¼å¯¹è¯**ï¼šæ”¯æŒå¤šè½®å¯¹è¯ï¼Œèƒ½å¤Ÿç†è§£ä¸Šä¸‹æ–‡å¹¶åšå‡ºç›¸åº”çš„å›ç­”ã€‚
4. **å®æ—¶åé¦ˆ**ï¼šç”¨æˆ·æå‡ºé—®é¢˜åï¼Œç³»ç»Ÿèƒ½å¤Ÿè¿…é€Ÿå“åº”å¹¶ç»™å‡ºç­”æ¡ˆã€‚

---

## åº”ç”¨ä»·å€¼

- **æ•™è‚²ç§‘ç ”**ï¼šä¸ºåœ°ç†å­¦æ•™è‚²å’Œç§‘ç ”å·¥ä½œæä¾›è¾…åŠ©å·¥å…·ï¼Œå¸®åŠ©å­¦ç”Ÿå’Œç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œå­¦ä¹ åœ°å½¢åœ°è²ŒçŸ¥è¯†ã€‚
- **å†³ç­–æ”¯æŒ**ï¼šä¸ºæ”¿åºœå’Œä¼äº‹ä¸šå•ä½æä¾›åœ°å½¢åœ°è²Œæ–¹é¢çš„å†³ç­–ä¾æ®ï¼Œè¾…åŠ©è§„åˆ’å’Œç®¡ç†ã€‚
- **å…¬ä¼—æ™®åŠ**ï¼šæé«˜å…¬ä¼—å¯¹åœ°ç†ç¯å¢ƒçš„è®¤è¯†æ°´å¹³ï¼Œä¿ƒè¿›ç¯å¢ƒä¿æŠ¤æ„è¯†çš„å½¢æˆã€‚

---

## æŠ€æœ¯æ–¹æ¡ˆ

ä¸ºäº†å®ç°ä¸Šè¿°åŠŸèƒ½ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä»¥ä¸‹æŠ€æœ¯æ–¹æ¡ˆï¼š

- **æ¨¡å‹é€‰æ‹©**ï¼šä½¿ç”¨IEIT Yuan 2-2B Mars-HFæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºTransformeræ¶æ„ï¼Œå…·æœ‰å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚
- **æ¡†æ¶æ”¯æŒ**ï¼šé‡‡ç”¨Streamlitæ¡†æ¶æ„å»ºå‰ç«¯ç•Œé¢ï¼Œå®ç°ç”¨æˆ·å‹å¥½çš„äº¤äº’ä½“éªŒã€‚
- **ç¼–ç¨‹è¯­è¨€**ï¼šä¸»è¦ä½¿ç”¨Pythonè¯­è¨€è¿›è¡Œå¼€å‘ã€‚
- **éƒ¨ç½²ç¯å¢ƒ**ï¼šå¯åœ¨æœ¬åœ°æœåŠ¡å™¨æˆ–äº‘å¹³å°ä¸Šéƒ¨ç½²ã€‚

---

## æ–¹æ¡ˆæ¶æ„

1. **å‰ç«¯ç•Œé¢**ï¼šä½¿ç”¨Streamlitæ­å»ºçš„ç”¨æˆ·ç•Œé¢ï¼ŒåŒ…æ‹¬è¾“å…¥æ¡†ã€æ¶ˆæ¯å†å²å±•ç¤ºåŒºç­‰ã€‚
2. **åç«¯é€»è¾‘**ï¼šåŒ…æ‹¬æ¨¡å‹åŠ è½½ã€è¾“å…¥å¤„ç†ã€ç”Ÿæˆå›ç­”ç­‰åŠŸèƒ½ã€‚
3. **æ¨¡å‹å±‚**ï¼šé¢„è®­ç»ƒçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å›ç­”ã€‚

---

## æ ¸å¿ƒä»£ç 
ä»£ç å·²ç»åŠ ä¸Šè¯¦ç»†æ³¨é‡Šï¼š
```python
# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸŒ åœ°å½¢åœ°è²Œæ™ºèƒ½é—®ç­”åŠ©æ‰‹")

# æºå¤§æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = './IEITYuan/Yuan2-2B-Mars-hf'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16 # A10

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("Creating tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)
    
    # è®¾å®šæˆ–æ·»åŠ å¡«å……token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # é€‰æ‹©å°†`<eod>`ä½œä¸ºå¡«å……token
        # æˆ–è€…æ·»åŠ ä¸€ä¸ªæ–°çš„tokenä½œä¸ºå¡«å……token
        # tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    
    print("Creating model...")
    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()

    print("Done.")
    return tokenizer, model

# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()

# åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# æ¯æ¬¡å¯¹è¯æ—¶ï¼Œéƒ½éœ€è¦éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # æ·»åŠ åœ°å½¢åœ°è²Œé¢†åŸŸçš„ä¸Šä¸‹æ–‡
    domain_context = "æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„åœ°ç†å­¦å®¶ï¼Œä¸“é—¨ç ”ç©¶åœ°å½¢åœ°è²Œã€‚"

    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # æ‹¼æ¥å¯¹è¯å†å²
    prompt = domain_context + "<n>".join(msg["content"] for msg in st.session_state.messages) + "<sep>"
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1024)

    # è·å–`input_ids`å’Œ`attention_mask`
    input_ids = inputs["input_ids"].cuda()
    attention_mask = inputs["attention_mask"].cuda()

    outputs = model.generate(input_ids, attention_mask=attention_mask, do_sample=False, max_new_tokens=1024) # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '')

    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
```

---

## ä»£ç ç»“æ„

1. **å¯¼å…¥å¿…éœ€æ¨¡å—**ï¼šä»`transformers`åº“å¯¼å…¥`AutoTokenizer`å’Œ`AutoModelForCausalLM`ç±»ï¼Œä»¥åŠä»`torch`åº“å¯¼å…¥`torch`ï¼Œè¿˜æœ‰`streamlit`åº“ç”¨äºåˆ›å»ºç”¨æˆ·ç•Œé¢ã€‚
2. **å‰ç«¯ç•Œé¢æ­å»º**ï¼šä½¿ç”¨`streamlit`åˆ›å»ºåŸºæœ¬çš„UIå…ƒç´ ï¼Œä¾‹å¦‚æ ‡é¢˜å’Œè¾“å…¥æ¡†ã€‚
3. **æ¨¡å‹ä¸Tokenizerçš„åŠ è½½**ï¼šå®šä¹‰ä¸€ä¸ªå‡½æ•°`get_model`æ¥åŠ è½½æ¨¡å‹å’Œtokenizerï¼Œå¹¶ç¼“å­˜è¿™ä¸ªè¿‡ç¨‹ä»¥é¿å…é‡å¤åŠ è½½ã€‚
4. **æ¨¡å‹æ¨ç†**ï¼šå®šä¹‰é€»è¾‘æ¥å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºï¼Œå¹¶æ˜¾ç¤ºç»“æœã€‚

### åŠŸèƒ½ä¸å®ç°é€»è¾‘

#### 1. å¯¼å…¥å¿…éœ€æ¨¡å—

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st
```

- **åŠŸèƒ½**ï¼šå¯¼å…¥æ‰€éœ€çš„åº“å’Œæ¨¡å—ã€‚
- **å®ç°é€»è¾‘**ï¼šè¿™äº›åº“æä¾›äº†æ¨¡å‹åŠ è½½ã€æ¨¡å‹æ¨ç†ä»¥åŠç”¨æˆ·ç•Œé¢åˆ›å»ºæ‰€éœ€çš„åŸºæœ¬åŠŸèƒ½ã€‚

#### 2. å‰ç«¯ç•Œé¢æ­å»º

```python
st.title("ğŸŒ åœ°å½¢åœ°è²Œæ™ºèƒ½é—®ç­”åŠ©æ‰‹")
```

- **åŠŸèƒ½**ï¼šè®¾ç½®åº”ç”¨çš„æ ‡é¢˜ã€‚
- **å®ç°é€»è¾‘**ï¼šä½¿ç”¨`streamlit`çš„`title`å‡½æ•°æ¥å®šä¹‰åº”ç”¨çš„ä¸»è¦æ ‡é¢˜ã€‚

#### 3. æ¨¡å‹ä¸Tokenizerçš„åŠ è½½

```python
from modelscope import snapshot_download
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')
path = model_dir

@st.cache_resource
def get_model():
    # ... (çœç•¥éƒ¨åˆ†ä»£ç )
    return tokenizer, model
```

- **åŠŸèƒ½**ï¼šä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ŒåŠ è½½æ¨¡å‹å’Œtokenizerã€‚

- **å®ç°é€»è¾‘**ï¼š
  - ä½¿ç”¨`modelscope`çš„`snapshot_download`å‡½æ•°ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚
  - ä½¿ç”¨`@st.cache_resource`è£…é¥°å™¨ç¡®ä¿æ¨¡å‹åªè¢«åŠ è½½ä¸€æ¬¡ã€‚
  - `AutoTokenizer`ç”¨äºåŠ è½½tokenizerï¼Œ`AutoModelForCausalLM`ç”¨äºåŠ è½½æ¨¡å‹ã€‚
  - æ‰©å±•tokenizerä»¥æ”¯æŒé¢å¤–çš„ç‰¹æ®Štokenã€‚

#### 4. æ¨¡å‹æ¨ç†

```python
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ... (çœç•¥éƒ¨åˆ†ä»£ç )

if prompt := st.chat_input():
    # ... (çœç•¥éƒ¨åˆ†ä»£ç )
    st.session_state.messages.append({"role": "user", "content": prompt})
    # ... (çœç•¥éƒ¨åˆ†ä»£ç )
    
    # æ‹¼æ¥å¯¹è¯å†å²
    prompt = domain_context + "\n".join(msg["content"] for msg in st.session_state.messages) + "<sep>"
    # ... (çœç•¥éƒ¨åˆ†ä»£ç )
    
    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
    outputs = model.generate(input_ids, attention_mask=attention_mask, do_sample=False, max_new_tokens=1024)
    output = tokenizer.decode(outputs[0])
    # ... (çœç•¥éƒ¨åˆ†ä»£ç )
```

- **åŠŸèƒ½**ï¼šå¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œå¹¶æ˜¾ç¤ºç»“æœã€‚

- **å®ç°é€»è¾‘**ï¼š
  - ä½¿ç”¨`st.session_state`æ¥å­˜å‚¨å’Œç»´æŠ¤å¯¹è¯å†å²ã€‚
  - ç”¨æˆ·é€šè¿‡`st.chat_input()`è¾“å…¥é—®é¢˜ã€‚
  - è¾“å…¥çš„é—®é¢˜è¢«æ·»åŠ åˆ°ä¼šè¯çŠ¶æ€(`st.session_state`)ä¸­ã€‚
  - è¾“å…¥é—®é¢˜å‰åŠ å…¥é¢†åŸŸä¸Šä¸‹æ–‡ã€‚
  - ä½¿ç”¨tokenizerå¯¹è¾“å…¥è¿›è¡Œç¼–ç ï¼Œå¹¶å°†ç¼–ç åçš„æ•°æ®ç§»è‡³GPUã€‚
  - è°ƒç”¨æ¨¡å‹çš„`generate`æ–¹æ³•ç”Ÿæˆå›ç­”ã€‚
  - å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œè§£ç ï¼Œå¹¶æå–æœ‰æ•ˆå›ç­”ã€‚
  - å°†æ¨¡å‹ç”Ÿæˆçš„å›ç­”æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Šã€‚
#### 5. åŠ è½½æ¨¡å‹å’Œtokenizer

```python
def get_model():
    print("Creating tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    # æ‰©å±•tokenizerï¼Œå¢åŠ ç‰¹æ®Štoken
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>', '<commit_before>', '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<empty_output>'], special_tokens=True)
    
    # è®¾å®šæˆ–æ·»åŠ å¡«å……token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # é€‰æ‹©å°†`<eod>`ä½œä¸ºå¡«å……token
    
    print("Creating model...")
    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()
    
    print("Done.")
    return tokenizer, model
```

- **åŠŸèƒ½**ï¼šåŠ è½½æ¨¡å‹å’Œtokenizerã€‚

- **å®ç°é€»è¾‘**ï¼š
- ä½¿ç”¨`AutoTokenizer.from_pretrained`åŠ è½½tokenizerã€‚
  - æ‰©å±•tokenizerä»¥æ”¯æŒé¢å¤–çš„ç‰¹æ®Štokenã€‚
- ä½¿ç”¨`AutoModelForCausalLM.from_pretrained`åŠ è½½æ¨¡å‹ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ°GPUä¸Šã€‚

#### 6. å¤„ç†ç”¨æˆ·è¾“å…¥å’Œç”Ÿæˆå›ç­”

```python
if prompt := st.chat_input():
    # ... (çœç•¥éƒ¨åˆ†ä»£ç )
    
    # æ‹¼æ¥å¯¹è¯å†å²
    prompt = domain_context + "\n".join(msg["content"] for msg in st.session_state.messages) + "<sep>"
    
    # å¯¹è¾“å…¥è¿›è¡Œç¼–ç 
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1024)
    
    # å°†è¾“å…¥æ•°æ®ç§»åˆ°GPUä¸Š
    input_ids = inputs["input_ids"].cuda()
    attention_mask = inputs["attention_mask"].cuda()
    
    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
    outputs = model.generate(input_ids, attention_mask=attention_mask, do_sample=False, max_new_tokens=1024)
    output = tokenizer.decode(outputs[0])
    
    # è§£ææ¨¡å‹çš„è¾“å‡º
    response = output.split("<sep>")[-1].replace("<eod>", '')
    
    # ... (çœç•¥éƒ¨åˆ†ä»£ç )
```

- **åŠŸèƒ½**ï¼šå¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”ã€‚
- **å®ç°é€»è¾‘**ï¼š
  - æ‹¼æ¥ç”¨æˆ·è¾“å…¥å’Œå¯¹è¯å†å²ã€‚
  - ä½¿ç”¨tokenizerå¯¹è¾“å…¥è¿›è¡Œç¼–ç ã€‚
  - å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°GPUä¸Šã€‚
  - ä½¿ç”¨æ¨¡å‹çš„`generate`æ–¹æ³•ç”Ÿæˆå›ç­”ã€‚
  - è§£ç æ¨¡å‹è¾“å‡ºï¼Œå¹¶æå–æœ‰æ•ˆå›ç­”ã€‚

---

## è¿è¡Œæ•ˆæœä¸é—®é¢˜

å‚è€ƒâ€œæ•ˆæœå›¾â€æ–‡ä»¶å¤¹ï¼Œé—®ç­”åŠ©æ‰‹å¯ä»¥æœ‰æ•ˆçš„å›ç­”åœ°ç†çŸ¥è¯†ï¼Œå¹¶è®¤ä¸ºè‡ªå·±æ˜¯å›ç­”åœ°ç†çŸ¥è¯†çš„ä¸“å®¶ã€‚å¯¹äºâ€œäºšé©¬é€Šï¼Œå–œé©¬æ‹‰é›…â€œè¿™æ ·æœ‰å¤šé‡å«ä¹‰çš„å®šä¹‰ï¼Œä¼šä¼˜å…ˆè§£é‡Šå…¶åœ°ç†å­¦ç›¸å…³çŸ¥è¯†ï¼Œä¸”å…·å¤‡å¤§æ¨¡å‹çš„ä¸€èˆ¬å¯¹è¯åŠŸèƒ½ã€‚

![æ•ˆæœå›¾-é—®ç­”æ•ˆæœ1](https://github.com/gishongji/ChatGEO/blob/main/%E6%95%88%E6%9E%9C%E5%9B%BE/%E6%95%88%E6%9E%9C%E5%9B%BE-%E9%97%AE%E7%AD%94%E6%95%88%E6%9E%9C1.png)

ä½†åœ¨â€å¾…ä¼˜åŒ–æƒ…å†µâ€œä¸­ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œæ¨¡å‹å­˜åœ¨â€å¿˜è®°â€œè‡ªå·±â€èº«ä»½â€œçš„æƒ…å†µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹æé—®æ—¶ï¼Œâ€äºšé©¬é€Šâ€œå°±ä¼šä»¥å…¬å¸çš„æƒ…å†µè¢«ç”¨ä½œç­”æ¡ˆï¼Œè€Œä¸æ˜¯åœ°ç†å­¦æ¦‚å¿µã€‚è¿™ç§æƒ…å†µæ˜¯æœ‰å¾…äºé€šè¿‡RAG LoRAç­‰æ–¹æ³•ä¼˜åŒ–æ¨¡å‹æ¥å®ç°çš„ï¼Œä¹Ÿæ˜¯æœ¬é¡¹ç›®ä»Šåè¦è€ƒè™‘çš„å†…å®¹ã€‚

---

## è¿œæœŸè®¡åˆ’

- **åŠŸèƒ½æ‰©å±•**ï¼šå¢åŠ æ›´å¤šåœ°ç†ç›¸å…³çš„åŠŸèƒ½ï¼Œå¦‚é…å›¾ã€æä¾›å‚è€ƒé“¾æ¥ç­‰ã€‚
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šæœªæ¥ç‰ˆæœ¬å°†æ”¯æŒå¤šç§è¯­è¨€ï¼Œæ»¡è¶³ä¸åŒåœ°åŒºç”¨æˆ·çš„éœ€æ±‚ã€‚
- **æŒç»­ä¼˜åŒ–**ï¼šæ ¹æ®ç”¨æˆ·åé¦ˆæŒç»­æ”¹è¿›æ¨¡å‹æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

---

## å¸‚åœºæ€è€ƒ

- **ç›®æ ‡ç”¨æˆ·**ï¼šä¸»è¦é¢å‘åœ°ç†å­¦ç ”ç©¶è€…ã€æ•™å¸ˆã€å­¦ç”Ÿä»¥åŠå¯¹åœ°å½¢åœ°è²Œæ„Ÿå…´è¶£çš„å…¬ä¼—ã€‚
- **å¸‚åœºéœ€æ±‚**ï¼šç›®å‰å¸‚åœºä¸Šç¼ºä¹ä¸“ä¸šçš„åœ°å½¢åœ°è²Œé—®ç­”åŠ©æ‰‹ï¼Œå­˜åœ¨è¾ƒå¤§çš„å¸‚åœºç©ºé—´ã€‚
- **ç«äº‰ä¼˜åŠ¿**ï¼šç»“åˆæœ€æ–°çš„AIæŠ€æœ¯å’Œä¸“ä¸šçš„é¢†åŸŸçŸ¥è¯†ï¼Œæä¾›é«˜è´¨é‡çš„æœåŠ¡ã€‚

---

## æ¨å¹¿ç­–ç•¥

- **å­¦æœ¯åˆä½œ**ï¼šä¸é«˜æ ¡å’Œç ”ç©¶æœºæ„åˆä½œï¼Œå…±åŒä¸¾åŠç ”è®¨ä¼šå’ŒåŸ¹è®­æ´»åŠ¨ã€‚
- **ç¤¾äº¤åª’ä½“å®£ä¼ **ï¼šé€šè¿‡å¾®åšã€çŸ¥ä¹ç­‰å¹³å°åˆ†äº«ä½¿ç”¨æ¡ˆä¾‹å’Œå¿ƒå¾—ï¼Œå¸å¼•æ›´å¤šæ½œåœ¨ç”¨æˆ·ã€‚
- **å…è´¹è¯•ç”¨**ï¼šæä¾›ä¸€å®šæœŸé™çš„å…è´¹è¯•ç”¨ï¼Œé¼“åŠ±ç”¨æˆ·å°è¯•å¹¶ç•™ä¸‹åé¦ˆã€‚
- **åˆä½œä¼™ä¼´**ï¼šä¸åœ°ç†ä¿¡æ¯ç›¸å…³çš„ä¼ä¸šå»ºç«‹åˆä½œå…³ç³»ï¼Œå…±åŒæ¨å¹¿äº§å“ã€‚

---

æœªå®Œå¾…ç»­Â·Â·Â·Â·Â·Â·